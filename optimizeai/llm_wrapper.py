# optimizeai/llm_wrapper.py
import dspy
import dsp
from optimizeai.config import Config
from optimizeai.cot import CoT

# Define the signature class with Chain of Thought
class ZeroShotQAWithCoT(dspy.Signature):
    """Optimize the function to reduce execution time, memory usage, and improve overall efficiency.
    Provide short detailed technical tips and refactor suggestions, such as algorithm improvements,
    data structure optimizations, or parallelization strategies if needed."""
    code = dspy.InputField(desc="Code to be optimized")
    perf_metrics = dspy.InputField(desc="""Performance metrics of the code along with the
                                   output of the code execution""")
    optimization = dspy.OutputField(desc="""Detailed solution to how the code can be optimized with insights on performance metrics
                                    and no code in a report form in a markdown format""")

class LLMWrapper:
    """Wrapper class for the Language Model (LLM) to interact with the model based on the configuration provided."""
    def __init__(self, config: Config):
        self.llm_name = config.llm
        self.api_key = config.key
        self.model_name = config.model
        self.mode = "offline"
        self.chain=""
        self.__setup_llm()

    def __setup_llm(self):
        """Setup the LLM model based on the configuration provided."""
        if self.llm_name == "google":
            self.llm = dspy.Google(model=self.model_name, api_key=self.api_key)
        elif self.llm_name == "openai":
            self.llm = dspy.OpenAI(model=self.model_name, api_key=self.api_key)
        elif self.llm_name == "anthropic":
            self.llm = dsp.anthropic.Claude(model=self.model_name, api_key=self.api_key)
        elif self.llm_name == "huggingface" and self.mode=="offline":
            self.llm = dspy.HFModel(model=self.model_name)
        elif self.llm_name == "ollama" and self.mode=="offline":
            self.llm = dspy.OllamaLocal(model=self.model_name, max_tokens=500, timeout_s=300)
        else:
            raise ValueError(f"Unsupported LLM: {self.llm_name}")

        dspy.settings.configure(lm=self.llm)
        self.chain = CoT(ZeroShotQAWithCoT)

    def send_request(self, code, perf_metrics):
        """Send a request to the LLM model with the given prompt and performance metrics.
        Args: code (str): The code to send to the LLM model.
              perf_metrics (str): The performance metrics to send to the LLM model.
        Returns: answer (str): The answer generated by the LLM model."""
        answer = self.chain.forward(code=code, perf_metrics=perf_metrics)
        return answer.optimization
