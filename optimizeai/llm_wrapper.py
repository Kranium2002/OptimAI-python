# optimizeai/llm_wrapper.py
import os
import dspy
from dotenv import load_dotenv
import dsp
from optimizeai.config import Config

load_dotenv()

# Define the signature class with Chain of Thought
class ZeroShotQAWithCoT(dspy.Signature):
    """Optimize the function to reduce execution time, memory usage, and improve overall efficiency.
    Provide short detailed technical tips and refactor suggestions, such as algorithm improvements,
    data structure optimizations, or parallelization strategies if needed."""
    code = dspy.InputField(desc="Code to be optimized")
    perf_metrics = dspy.InputField(desc="Performance metrics of the code along with the output of the code execution")
    answer = dspy.OutputField(desc="The answer to how the code can be optimized")
    rationale = dspy.OutputField(desc="The rationale behind the optimizations")

class LLMWrapper:
    """Wrapper class for the Language Model (LLM) to interact with the model based on the configuration provided."""
    def __init__(self, config: Config):
        self.llm_name = config.llm
        self.api_key = config.key
        self.model_name = config.model
        self.mode = "offline"
        self.__setup_llm()

    def __setup_llm(self):
        """Setup the LLM model based on the configuration provided."""
        if self.llm_name == "google":
            self.llm = dspy.Google(model=self.model_name, api_key=self.api_key)
        elif self.llm_name == "openai":
            self.llm = dspy.OpenAI(model=self.model_name, api_key=self.api_key)
        elif self.llm_name == "anthropic":
            self.llm = dsp.anthropic.Claude(model=self.model_name, api_key=self.api_key)
        elif self.llm_name == "huggingface" and self.mode=="offline":
            self.llm = dspy.HFModel(model=self.model_name)
        else:
            raise ValueError(f"Unsupported LLM: {self.llm_name}")

        dspy.settings.configure(lm=self.llm)

    def send_request(self, code, perf_metrics):
        """Send a request to the LLM model with the given prompt and performance metrics.
        Args: code (str): The code to send to the LLM model.
              perf_metrics (str): The performance metrics to send to the LLM model.
        Returns: answer (str): The answer generated by the LLM model."""
        predictor = dspy.ChainOfThought(ZeroShotQAWithCoT)
        prediction = predictor(code=code, perf_metrics=perf_metrics)
        return prediction.answer, prediction.rationale
